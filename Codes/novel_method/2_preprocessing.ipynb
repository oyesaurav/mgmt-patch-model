{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import libraries and define variables\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "import pandas as pd\n",
    "from multiprocessing import Process, current_process\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")  # Adds the parent directory to sys.path\n",
    "import config \n",
    "\n",
    "# Define the modalities and classifications\n",
    "# modalities = ['T1', 'T1GD', 'T2', 'FLAIR']\n",
    "modalities = ['T1']\n",
    "classifications = ['MGMT_positive', 'MGMT_negative']\n",
    "\n",
    "# Define patch size and stride\n",
    "block_h, block_w = config.PATCH_SIZE\n",
    "stride = 2\n",
    "\n",
    "# Interpolated image dimestions\n",
    "inter_dim = (110, 90)\n",
    "\n",
    "# Define paths to the BraTS dataset folders\n",
    "path = config.MAIN_DIR\n",
    "\n",
    "Preprocess_Dir = path + 'Preprocessed/layers/'\n",
    "PATH = path + 'Data/'\n",
    "Org_Dir = PATH + 'Original_Data_Backup/'\n",
    "Work_Dir = PATH + 'Working_Data/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### One Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(config.MAIN_DIR+'results/train_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_data (64, 2)\n",
      "Shape of test_data (16, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mgmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>UPENN-GBM-00494_11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>UPENN-GBM-00034_11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>UPENN-GBM-00124_11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>UPENN-GBM-00312_11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>71</th>\n",
       "      <td>UPENN-GBM-00442_11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    id  mgmt\n",
       "41  UPENN-GBM-00494_11     1\n",
       "10  UPENN-GBM-00034_11     0\n",
       "11  UPENN-GBM-00124_11     1\n",
       "12  UPENN-GBM-00312_11     0\n",
       "71  UPENN-GBM-00442_11     1"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Spliting train to train and val\n",
    "# train1_data => Train data fixed for one split\n",
    "# val1_data => val data fixed for one split\n",
    "train1_data,val1_data=train_test_split(train_data[['id','mgmt']],\n",
    "                                      stratify=train_data['mgmt'],\n",
    "                                      random_state=100,\n",
    "                                      test_size=0.2\n",
    "                                      )\n",
    "print(f'Shape of train_data {train1_data.shape}')\n",
    "print(f'Shape of test_data {val1_data.shape}')\n",
    "train1_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions --> For reading images and storing all patches of a patient in a pickle file\n",
    "\n",
    "# def patches_to_pickle(mgmt_type):\n",
    "#     print('Reading Images')\n",
    "#     workdir = os.listdir(Work_Dir)\n",
    "#     # for mgmt_type in workdir:\n",
    "#     for patient in os.listdir(Work_Dir + mgmt_type + '/'):\n",
    "#         patient_patches = []\n",
    "#         for patch in tqdm(os.listdir(Work_Dir + mgmt_type + '/' + patient + '/')):\n",
    "#             try:\n",
    "#                 img_array = cv2.imread(os.path.join(Work_Dir, mgmt_type+'/'+patient+'/'+patch), cv2.IMREAD_GRAYSCALE)\n",
    "#                 patient_patches.append(img_array)\n",
    "#             except Exception as e:\n",
    "#                 print(e)\n",
    "#         print(patient + \" ✔\")\n",
    "#         pickle.dump(patient_patches, open(Preprocess_Dir + mgmt_type + '/' + patient, 'wb'))\n",
    "#     print(mgmt_type + \" ✔✔\")\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import preprocess_worker\n",
    "\n",
    "# stoer paths of patients whose patches have been made\n",
    "def img_to_npy(df: pd.DataFrame,selection: str,type: str):\n",
    "    patient_paths = [] \n",
    "    os.chdir(Work_Dir)\n",
    "    for i in tqdm(range(len(df))):\n",
    "        cls = 'MGMT_negative' if df['mgmt'].iloc[i] == 0 else 'MGMT_positive'\n",
    "        patient_path = cls + '/' + df['id'].iloc[i]\n",
    "        pool_imgs = os.listdir('./'+patient_path)\n",
    "        # print(patient_path+'/'+pool_imgs[0])\n",
    "    \n",
    "        for path in pool_imgs:\n",
    "            if selection in path:\n",
    "                patient_paths.append('./'+patient_path+'/'+path)\n",
    "\n",
    "\n",
    "    print(\"T2 Pateints paths are selected\",len(patient_paths))\n",
    "    # print(patient_paths[0:10])\n",
    "    if __name__ ==  '__main__': \n",
    "        num_processors = 15\n",
    "        p=Pool(processes = num_processors)\n",
    "        res = p.map(preprocess_worker.worker,[pat_id for pat_id in tqdm(patient_paths)])\n",
    "        pos_arr = []\n",
    "        neg_arr = []\n",
    "        for arr, path in res:\n",
    "            if 'MGMT_positive' in path:\n",
    "                pos_arr.append(arr)\n",
    "\n",
    "            elif 'MGMT_neg' in path:\n",
    "                neg_arr.append(arr)\n",
    "        pos_store_path = f'D:/MGMT research project/data for one split/Patch 64x64/{selection}/pos_one_split_{type}_data.npy'\n",
    "        neg_store_path = f'D:/MGMT research project/data for one split/Patch 64x64/{selection}/neg_one_split_{type}_data.npy'\n",
    "        np.save(pos_store_path,pos_arr)\n",
    "        np.save(neg_store_path,neg_arr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [02:37<00:00,  2.47s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2 Pateints paths are selected 1176672\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1176672/1176672 [00:00<00:00, 2463376.23it/s]\n",
      "100%|██████████| 16/16 [00:42<00:00,  2.64s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "T2 Pateints paths are selected 326592\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 326592/326592 [00:00<00:00, 1899984.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# Converting train patients in npy\n",
    "img_to_npy(train1_data,selection='T2',type='train')\n",
    "\n",
    "# Converting val patients in npy\n",
    "img_to_npy(val1_data,selection='T2',type='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storing train1_data and val1_data into a pickle file\n",
    "def img_to_npy(df: pd.DataFrame, selection: str,type: str):\n",
    "    os.chdir(Work_Dir)\n",
    "    res=np.array([])\n",
    "    for i in tqdm(range(df.shape[0])):\n",
    "        patient_id = df['id'].iloc[i]\n",
    "        cls = 'MGMT_positive' if df['mgmt'].iloc[i]==1 else 'MGMT_negative'\n",
    "        patient_path = os.path.join(cls,patient_id)\n",
    "        pool_img = os.listdir(patient_path)\n",
    "        # print(pool_img)\n",
    "        for img in pool_img:\n",
    "                if selection in img:\n",
    "                    img_path = f'./{cls}/{patient_id}/{img}'\n",
    "                    x = cv2.imread(img_path)\n",
    "                    if len(res)==0:\n",
    "                         res = np.array([x])\n",
    "                    \n",
    "                    else:\n",
    "                         res = np.append(res,np.array([x]),axis=0)\n",
    "                         \n",
    "        # print(res)\n",
    "        # Storing the data pkl format\n",
    "        storing_path = f'D:/MGMT research project/data for one split/Patch 64x64/{selection}/{cls}_one_split_{type}_data.npy'\n",
    "        # print(storing_path)\n",
    "        np.save(storing_path,res)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### image to array pickle, split during train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from multiprocessing import Pool\n",
    "import preprocess_worker\n",
    "\n",
    "# stoer paths of patients whose patches have been made\n",
    "df = pd.read_csv(path + \"Codes/upenn_data.csv\")\n",
    "patient_paths = []\n",
    "for i in tqdm(range(len(df))):\n",
    "    if(df.patches64[i] == True):\n",
    "        patient_path = ('MGMT_negative' if df.mgmt[i] == 0 else 'MGMT_positive') + '/' + df.id[i]\n",
    "        patient_paths.append(patient_path)\n",
    "\n",
    "print(len(patient_paths))\n",
    "\n",
    "if __name__ ==  '__main__': \n",
    " num_processors = 15\n",
    " p=Pool(processes = num_processors)\n",
    " p.map(preprocess_worker.worker,[pat_id for pat_id in tqdm(patient_paths)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions --> For reading images and appending it to a list\n",
    "\n",
    "def read_image(data):\n",
    "    print('Reading Images')\n",
    "    class_num = 0\n",
    "    workdir = os.listdir(Work_Dir)\n",
    "    if '.DS_Store' in workdir:\n",
    "          workdir.remove('.DS_Store')\n",
    "          print('Removed .DS_Store')\n",
    "    for classi in classifications:\n",
    "        if classi in workdir:\n",
    "            workdir.remove(classi)\n",
    "    for pool in workdir:\n",
    "        pool_dir = Work_Dir + pool + '/'\n",
    "        pool_dir_list = os.listdir(pool_dir)\n",
    "        if '.DS_Store' in pool_dir_list:\n",
    "            pool_dir_list.remove('.DS_Store')\n",
    "            print('Removed .DS_Store')\n",
    "        # i = 0\n",
    "        for img in tqdm(pool_dir_list):\n",
    "            # i += 1\n",
    "            # if(i == 100): break\n",
    "            try:\n",
    "                img_array = cv2.imread(os.path.join(pool_dir, img), cv2.IMREAD_GRAYSCALE)\n",
    "                # Saving images in the list\n",
    "                data.append([img_array, class_num])\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "        class_num = 1\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definitions --> Initialize the feature & labels of the processes image in the list X & Y\n",
    "\n",
    "def Initializing_feature_labels(data, X, Y):\n",
    "    print('Initializing Features & Labels')\n",
    "    for features, label in data:\n",
    "        X.append(features)\n",
    "        Y.append(label)\n",
    "    print('List Size: ', len(X), len(Y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Defination --> Reshape the list to numpy array\n",
    "\n",
    "def Converting(block_h, block_w, X, Y):\n",
    "    print('Converting to Array')\n",
    "    global x, y\n",
    "\n",
    "    # -1 is added to solve dimension mismatch while converting list to an array.\n",
    "    x = np.array(X).reshape((-1, block_h, block_w, 1))\n",
    "    y = np.array(Y)\n",
    "\n",
    "    print('Array Size with Reshape: ', len(X), len(y))\n",
    "    print('Array Shape with Reshape: ', x.shape, y.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main cell to execute all the functions\n",
    "\n",
    "# # Creating list for storing processed data\n",
    "# data = []\n",
    "\n",
    "# # Reading Images\n",
    "# read_image(data) \n",
    "\n",
    "# #  Printing the length of the data\n",
    "# print('Size of the data: ', len(data))\n",
    "\n",
    "# # Initializing all features & labels of the processed image in the list X & Y\n",
    "# X = []\n",
    "# Y = []\n",
    "\n",
    "# # Initializing the features and labels\n",
    "# Initializing_feature_labels(data, X, Y)\n",
    "\n",
    "# # Converting the list into numpy array\n",
    "# Converting(block_h, block_w, X, Y)\n",
    "\n",
    "# # Storing the numpy array in a pickle file\n",
    "# Storing_Preprocessed_Data = open(Work_Dir + 'X.pickle', 'wb')\n",
    "# pickle.dump(X, Storing_Preprocessed_Data)\n",
    "# Storing_Preprocessed_Data.close()\n",
    "\n",
    "# Storing_Preprocessed_Data = open(Work_Dir + 'y.pickle', 'wb')\n",
    "# pickle.dump(y, Storing_Preprocessed_Data)\n",
    "# Storing_Preprocessed_Data.close()\n",
    "\n",
    "\n",
    "patches_to_pickle()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying dataframe in place of imgarray\n",
    "\n",
    "def create_dataframe():\n",
    "    modality_in_annotated = sorted(os.listdir(Work_Dir))\n",
    "\n",
    "    image=[]\n",
    "    label=[]\n",
    "\n",
    "    for classi in classifications:\n",
    "        if classi in modality_in_annotated:\n",
    "            modality_in_annotated.remove(classi)\n",
    "                \n",
    "    for pool_modality in modality_in_annotated:\n",
    "        for img in tqdm(os.listdir(Work_Dir + pool_modality + '/')):\n",
    "            image.append(Work_Dir + pool_modality + '/' + img)\n",
    "            label.append(0 if 'MGMT_negative' in pool_modality else 1) \n",
    "\n",
    "    df = pd.DataFrame()\n",
    "    df['images']=[str(x) for x in image]\n",
    "    df['labels']=[str(x) for x in label]\n",
    "    # df = df.sample(frac=1, random_state=1).reset_index(drop=True)\n",
    "    df.to_csv('step2_file_paths.csv')\n",
    "    df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create_dataframe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Layer wise interpolated images Preprocessing/layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 2009.73it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "149\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149/149 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Pool\n",
    "import preprocess_worker\n",
    "\n",
    "patient_paths = []\n",
    "\n",
    "for type in tqdm(os.listdir(Work_Dir)):\n",
    "   for mod in modalities:\n",
    "      for patient in os.listdir(Work_Dir + type + '/' + mod + '/'):\n",
    "        patient_path = type + '/' + mod + '/' + patient\n",
    "        patient_paths.append(patient_path)\n",
    "\n",
    "print(len(patient_paths))\n",
    "\n",
    "if __name__ ==  '__main__': \n",
    " num_processors = 15\n",
    " p=Pool(processes = num_processors)\n",
    " p.map(preprocess_worker.worker,[pat_id for pat_id in tqdm(patient_paths)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b9603ab75704494434c4abe56997ef3bb46c839483ec68f7dba80f8b5009106"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
