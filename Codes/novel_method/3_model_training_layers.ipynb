{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import libraries and define variables\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import cv2\n",
    "import random as rn\n",
    "from multiprocessing import Pool,Process\n",
    "import config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dense, Flatten, Dropout, LeakyReLU, Activation, AveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "from keras.layers.convolutional import Conv2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import TruePositives,TrueNegatives,FalsePositives,FalseNegatives,AUC,Recall,Precision\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping,TensorBoard\n",
    "from tensorflow.keras.regularizers import l2\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "# Define the modalities and classifications\n",
    "modalities = ['T1']\n",
    "classifications = ['MGMT_positive', 'MGMT_negative']\n",
    "\n",
    "# Define patch size and stride\n",
    "block_h, block_w = config.PATCH_SIZE\n",
    "stride = 2\n",
    "\n",
    "# Interpolated image dimestions\n",
    "inter_dim = (110, 90)\n",
    "\n",
    "# Define epoch\n",
    "epoch = 100\n",
    "batch_size = 16\n",
    "\n",
    "# Define paths to the BraTS dataset folders\n",
    "path = config.MAIN_DIR\n",
    "\n",
    "PATH = config.MAIN_DIR + 'Data/'\n",
    "Org_Dir = PATH + 'Original_Data_Backup/'\n",
    "Work_Dir = PATH + 'Working_Data/'\n",
    "Preprocess_Dir = path + 'Preprocessed/layers/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00, 39.98it/s]\n"
     ]
    }
   ],
   "source": [
    "# df = pd.DataFrame(columns=['ID', 'MGMT', 'Mod'])\n",
    "rows =[]\n",
    "for type in tqdm(os.listdir(Work_Dir)):\n",
    "   for mod in modalities:\n",
    "      for patient in os.listdir(Work_Dir + type + '/' + mod + '/'):\n",
    "        new_row = { 'ID' : patient, 'MGMT' : (0 if type == 'MGMT_negative' else 1), 'Mod' : mod }\n",
    "        rows.append(new_row)\n",
    "\n",
    "df = pd.DataFrame(rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_arr(train, val):\n",
    "    train_x, train_y = [], []\n",
    "    val_x, val_y = [], []\n",
    "    for i in range(len(train)):\n",
    "        type = 'MGMT_negative' if train['MGMT'].iloc[i] == 0 else 'MGMT_positive'\n",
    "        pkl_file = pkl.load(open(config.MAIN_DIR+'preprocessed/layers/'+type+'/T1/'+train['ID'].iloc[i],'rb'))\n",
    "        for arr in pkl_file:\n",
    "            train_x.append(arr)\n",
    "            train_y.append(train['MGMT'].iloc[i])\n",
    "    for i in range(len(val)):\n",
    "        type = 'MGMT_negative' if val['MGMT'].iloc[i] == 0 else 'MGMT_positive'\n",
    "        pkl_file = pkl.load(open(config.MAIN_DIR+'preprocessed/layers/'+type+'/T1/'+val['ID'].iloc[i],'rb'))\n",
    "        for arr in pkl_file:\n",
    "            val_x.append(arr)\n",
    "            val_y.append(val['MGMT'].iloc[i])\n",
    "\n",
    "    # train_x = np.array(train_x)/ 255.0\n",
    "    # val_x = np.array(val_x)/ 255.0\n",
    "    print('data_arr done')\n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_model () :\n",
    "        model = Sequential()\n",
    "\n",
    "        # model.add(Conv2D(16, (5,5), padding='same',input_shape=(90,110,1),kernel_initializer=HeNormal()))\n",
    "        # model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        # model.add(BatchNormalization())\n",
    "        # model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Conv2D(4, (3, 3), padding='same',input_shape=(90,110,1),kernel_initializer=HeNormal(),kernel_regularizer=l2(0.01)))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        # model.add(BatchNormalization())\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        # model.add(Conv2D(8, (3, 3), padding='same',kernel_initializer=HeNormal()))\n",
    "        # model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        # model.add(BatchNormalization())\n",
    "        # model.add(Dropout(0.))\n",
    "\n",
    "        model.add(Conv2D(2, (3, 3), padding='same',kernel_initializer=HeNormal(),kernel_regularizer=l2(0.01)))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        # model.add(BatchNormalization())\n",
    "        # model.add(Dropout(0.1))\n",
    "        \n",
    "        # model.add(Conv2D(2, (3, 3), padding='same',kernel_initializer=HeNormal(),kernel_regularizer=l2(0.01)))\n",
    "        # model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        # model.add(BatchNormalization())\n",
    "        # # model.add(Dropout(0.1))\n",
    "\n",
    "\n",
    "        # model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "        # model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "        # model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Flatten())  # Convert 3D feature map to 1D feature vector.\n",
    "\n",
    "        # model.add(Dense(10,kernel_initializer=HeNormal()))\n",
    "        # model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(BatchNormalization())\n",
    "        # model.add(Dropout(0.3))\n",
    "\n",
    "        # model.add(Dense(10,kernel_initializer=HeNormal()))\n",
    "        # model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(BatchNormalization())\n",
    "        # model.add(Dropout(0.2))\n",
    "        \n",
    "        # model.add(Dense(10,kernel_initializer=HeNormal()))\n",
    "        # model.add(BatchNormalization())\n",
    "        # model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Dense(10,kernel_initializer=HeNormal(),kernel_regularizer=l2(0.01)))\n",
    "        model.add(BatchNormalization())\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(Dropout(0.1))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam', \n",
    "                    metrics=['accuracy',TruePositives(),\n",
    "                             TrueNegatives(),FalsePositives(),\n",
    "                             FalseNegatives(),AUC(),Recall(),Precision()])\n",
    "        return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Funtion Defination --> train the model and stores the history\n",
    "# import h5py\n",
    "def model_training(cv, train, val):\n",
    "\n",
    "    # Model intialisation with GPU\n",
    "    with tf.device('GPU:0'):\n",
    "        model = define_model()\n",
    "\n",
    "        # Compiling the model\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy',TruePositives(),TrueNegatives(),FalsePositives(),FalseNegatives(),AUC(),Recall(),\n",
    "                                                Precision()])\n",
    "\n",
    "    # Selecting the data from train_idx and test_idx\n",
    "    X_train, y_train, X_val, y_val = data_arr(train,val)\n",
    "    print(\"all data fetched\")\n",
    "    X_train=np.array(X_train)/255.0\n",
    "    y_train=np.array(y_train)\n",
    "    X_val=np.array(X_val)/255.0\n",
    "    y_val=np.array(y_val)\n",
    "    # print(y_train[:50])\n",
    "    print('Train shape: ',X_train.shape)\n",
    "    print('Val shape: ',X_val.shape)\n",
    "    # Model Checkpoints\n",
    "    checkpoint_filepath = config.MAIN_DIR+f'results/model checkpoints/model(k={cv})'\n",
    "    model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath+'_epoch-{epoch:02d}_acc-{val_accuracy:.4f}.ckpt',\n",
    "                                                # save_format='h5',\n",
    "                                                monitor='val_loss', \n",
    "                                                mode='min',\n",
    "                                                save_best_only=True)\n",
    "    # Reduce LRPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=0.0001)\n",
    "\n",
    "    # Early stoping\n",
    "    early_stoping =EarlyStopping(monitor=\"val_loss\",patience=5,mode=\"min\") \n",
    "\n",
    "    # Tensorboard\n",
    "    tensorboard = TensorBoard(f'D:/MGMT research project/tensorboard/logdir_{cv}', histogram_freq=1)\n",
    "    # Model training\n",
    "    print(f\"Model Training for {cv} was started...\")\n",
    "    \n",
    "    history = model.fit(X_train, y_train, batch_size=8, epochs=epoch,\n",
    "                        validation_data=(X_val, y_val,), shuffle=True,callbacks=[reduce_lr,early_stoping, model_checkpoint_callback])\n",
    "    \n",
    "    # Stores the history in pickle\n",
    "    # pkl.dump(history.history,open(config.MAIN_DIR+f'results/history/history_k={cv}.pkl','wb'))\n",
    "    print(f\"Model Training for cv-{cv} was completed....\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_arr done\n",
      "all data fetched\n",
      "Train shape:  (6808, 90, 110)\n",
      "Val shape:  (1641, 90, 110)\n",
      "Model Training for 1 was started...\n",
      "Epoch 1/100\n",
      "851/851 [==============================] - ETA: 0s - loss: 0.9380 - accuracy: 0.5981 - true_positives_24: 1833.0000 - true_negatives_24: 2239.0000 - false_positives_24: 1162.0000 - false_negatives_24: 1574.0000 - auc_24: 0.6365 - recall_24: 0.5380 - precision_24: 0.6120"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 2 of 2). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/MGMT research project/results/model checkpoints\\model(k=1)_epoch-01_acc-0.4851.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/MGMT research project/results/model checkpoints\\model(k=1)_epoch-01_acc-0.4851.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "851/851 [==============================] - 26s 29ms/step - loss: 0.9380 - accuracy: 0.5981 - true_positives_24: 1833.0000 - true_negatives_24: 2239.0000 - false_positives_24: 1162.0000 - false_negatives_24: 1574.0000 - auc_24: 0.6365 - recall_24: 0.5380 - precision_24: 0.6120 - val_loss: 0.9249 - val_accuracy: 0.4851 - val_true_positives_24: 150.0000 - val_true_negatives_24: 646.0000 - val_false_positives_24: 203.0000 - val_false_negatives_24: 642.0000 - val_auc_24: 0.4772 - val_recall_24: 0.1894 - val_precision_24: 0.4249 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.8125 - accuracy: 0.6426 - true_positives_24: 2261.0000 - true_negatives_24: 2114.0000 - false_positives_24: 1287.0000 - false_negatives_24: 1146.0000 - auc_24: 0.6880 - recall_24: 0.6636 - precision_24: 0.6373 - val_loss: 1.0590 - val_accuracy: 0.4040 - val_true_positives_24: 340.0000 - val_true_negatives_24: 323.0000 - val_false_positives_24: 526.0000 - val_false_negatives_24: 452.0000 - val_auc_24: 0.3829 - val_recall_24: 0.4293 - val_precision_24: 0.3926 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.7965 - accuracy: 0.6620 - true_positives_24: 2316.0000 - true_negatives_24: 2191.0000 - false_positives_24: 1210.0000 - false_negatives_24: 1091.0000 - auc_24: 0.7116 - recall_24: 0.6798 - precision_24: 0.6568 - val_loss: 0.9855 - val_accuracy: 0.5058 - val_true_positives_24: 135.0000 - val_true_negatives_24: 695.0000 - val_false_positives_24: 154.0000 - val_false_negatives_24: 657.0000 - val_auc_24: 0.4336 - val_recall_24: 0.1705 - val_precision_24: 0.4671 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.8013 - accuracy: 0.6429 - true_positives_24: 2232.0000 - true_negatives_24: 2145.0000 - false_positives_24: 1256.0000 - false_negatives_24: 1175.0000 - auc_24: 0.6984 - recall_24: 0.6551 - precision_24: 0.6399 - val_loss: 0.9480 - val_accuracy: 0.4650 - val_true_positives_24: 287.0000 - val_true_negatives_24: 476.0000 - val_false_positives_24: 373.0000 - val_false_negatives_24: 505.0000 - val_auc_24: 0.4473 - val_recall_24: 0.3624 - val_precision_24: 0.4348 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "851/851 [==============================] - 22s 26ms/step - loss: 0.7004 - accuracy: 0.7158 - true_positives_24: 2509.0000 - true_negatives_24: 2364.0000 - false_positives_24: 1037.0000 - false_negatives_24: 898.0000 - auc_24: 0.7859 - recall_24: 0.7364 - precision_24: 0.7076 - val_loss: 0.9383 - val_accuracy: 0.4674 - val_true_positives_24: 495.0000 - val_true_negatives_24: 272.0000 - val_false_positives_24: 577.0000 - val_false_negatives_24: 297.0000 - val_auc_24: 0.4316 - val_recall_24: 0.6250 - val_precision_24: 0.4618 - lr: 2.0000e-04\n",
      "Epoch 6/100\n",
      "851/851 [==============================] - 23s 27ms/step - loss: 0.6331 - accuracy: 0.7519 - true_positives_24: 2620.0000 - true_negatives_24: 2499.0000 - false_positives_24: 902.0000 - false_negatives_24: 787.0000 - auc_24: 0.8267 - recall_24: 0.7690 - precision_24: 0.7439 - val_loss: 0.9606 - val_accuracy: 0.4595 - val_true_positives_24: 220.0000 - val_true_negatives_24: 534.0000 - val_false_positives_24: 315.0000 - val_false_negatives_24: 572.0000 - val_auc_24: 0.4377 - val_recall_24: 0.2778 - val_precision_24: 0.4112 - lr: 2.0000e-04\n",
      "Model Training for cv-1 was completed....\n"
     ]
    }
   ],
   "source": [
    "skf = StratifiedKFold(n_splits=5, random_state=100, shuffle=True)\n",
    "\n",
    "i = 0\n",
    "for train_idx, val_idx in skf.split(df['ID'], df['MGMT']):\n",
    "    train = df.iloc[train_idx]\n",
    "    val = df.iloc[val_idx]\n",
    "    i = i+1\n",
    "    # train.to_csv('splits/train_' + str(i) + '.csv', index=False)\n",
    "    # val.to_csv('splits/val_' + str(i) + '.csv', index=False)\n",
    "\n",
    "    model_training(i, train, val)\n",
    "    if i == 1:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data_arr done\n",
      "52/52 [==============================] - 1s 13ms/step\n",
      "Accuracy on 1 :  0.52224253503961\n",
      "data_arr done\n",
      "54/54 [==============================] - 1s 9ms/step\n",
      "Accuracy on 2 :  0.5223188405797101\n",
      "data_arr done\n",
      "54/54 [==============================] - 1s 10ms/step\n",
      "Accuracy on 3 :  0.6509711595055915\n",
      "data_arr done\n",
      "54/54 [==============================] - 1s 10ms/step\n",
      "Accuracy on 4 :  0.5833333333333334\n"
     ]
    }
   ],
   "source": [
    "# Checking the best model for other 4 splits\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "best_model_k = 5\n",
    "\n",
    "model = define_model()\n",
    "model.load_weights(\"D:\\MGMT research project\\Codes\\model(k=5)_epoch-01_acc-0.5274.ckpt\")\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "for i in range(5):\n",
    "    if(i+1 != best_model_k) : \n",
    "        train = pd.read_csv('splits/train_' + str(i+1) + '.csv')\n",
    "        val = pd.read_csv('splits/val_' + str(i+1) + '.csv')\n",
    "        \n",
    "        X_train, y_train, X_val, y_val = data_arr(train,val)\n",
    "\n",
    "        X_val=np.array(X_val)/255.0\n",
    "        y_val=np.array(y_val)\n",
    "\n",
    "        output = model.predict(X_val)\n",
    "        output = np.where(output.squeeze()>0.5,1,0)\n",
    "        print(\"Accuracy on \" + str(i+1)+ \" : \", accuracy_score(y_val, output))\n",
    "    \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tfgpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
