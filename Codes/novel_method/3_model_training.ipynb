{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import libraries and define variables\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle as pkl\n",
    "import cv2\n",
    "import random as rn\n",
    "from multiprocessing import Pool,Process\n",
    "import config\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dense, Flatten, Dropout, LeakyReLU, Activation, AveragePooling2D, BatchNormalization\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.metrics import TruePositives,TrueNegatives,FalsePositives,FalseNegatives,AUC,Recall,Precision\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint,ReduceLROnPlateau,EarlyStopping\n",
    "import time\n",
    "import shutil\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "\n",
    "# Define the modalities and classifications\n",
    "modalities = ['T1', 'T1GD', 'T2', 'FLAIR']\n",
    "classifications = ['MGMT_positive', 'MGMT_negative']\n",
    "\n",
    "# Define patch size and stride\n",
    "block_h, block_w = config.PATCH_SIZE\n",
    "stride = 2\n",
    "\n",
    "# Interpolated image dimestions\n",
    "inter_dim = (110, 90)\n",
    "\n",
    "# Define epoch\n",
    "epoch = 100\n",
    "batch_size = 128\n",
    "\n",
    "# Define paths to the BraTS dataset folders\n",
    "path = config.MAIN_DIR\n",
    "\n",
    "PATH = config.MAIN_DIR + 'Data_100_64x64/'\n",
    "Org_Dir = PATH + 'Original_Data_Backup/'\n",
    "Work_Dir = PATH + 'Working_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>mgmt</th>\n",
       "      <th>age</th>\n",
       "      <th>gender</th>\n",
       "      <th>survival</th>\n",
       "      <th>patches</th>\n",
       "      <th>patches64</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>UPENN-GBM-00022_11</td>\n",
       "      <td>0</td>\n",
       "      <td>53.88</td>\n",
       "      <td>F</td>\n",
       "      <td>1882</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>UPENN-GBM-00034_11</td>\n",
       "      <td>0</td>\n",
       "      <td>53.63</td>\n",
       "      <td>F</td>\n",
       "      <td>464</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>UPENN-GBM-00088_11</td>\n",
       "      <td>0</td>\n",
       "      <td>47.32</td>\n",
       "      <td>M</td>\n",
       "      <td>334</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>UPENN-GBM-00091_11</td>\n",
       "      <td>0</td>\n",
       "      <td>70.54</td>\n",
       "      <td>M</td>\n",
       "      <td>200</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>UPENN-GBM-00093_11</td>\n",
       "      <td>0</td>\n",
       "      <td>51.30</td>\n",
       "      <td>F</td>\n",
       "      <td>616</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                  id  mgmt    age gender  survival  patches  \\\n",
       "0           0  UPENN-GBM-00022_11     0  53.88      F      1882     True   \n",
       "1           0  UPENN-GBM-00034_11     0  53.63      F       464     True   \n",
       "2           0  UPENN-GBM-00088_11     0  47.32      M       334     True   \n",
       "3           0  UPENN-GBM-00091_11     0  70.54      M       200     True   \n",
       "4           0  UPENN-GBM-00093_11     0  51.30      F       616     True   \n",
       "\n",
       "   patches64  \n",
       "0       True  \n",
       "1       True  \n",
       "2       True  \n",
       "3       True  \n",
       "4       True  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reading upenn_data.csv\n",
    "pat_df=pd.read_csv('../upenn_data.csv')\n",
    "\n",
    "# considering pateints id's where patches are True\n",
    "pat_df=pat_df[pat_df['patches']==True]\n",
    "pat_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Train Test Split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of train_data (80, 2)\n",
      "Shape of test_data (20, 2)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>mgmt</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>UPENN-GBM-00294_11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>116</th>\n",
       "      <td>UPENN-GBM-00445_11</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>UPENN-GBM-00132_11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>UPENN-GBM-00150_11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>UPENN-GBM-00093_11</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id  mgmt\n",
       "47   UPENN-GBM-00294_11     0\n",
       "116  UPENN-GBM-00445_11     1\n",
       "17   UPENN-GBM-00132_11     0\n",
       "33   UPENN-GBM-00150_11     0\n",
       "4    UPENN-GBM-00093_11     0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Splitting Data into train and test\n",
    "train_data,test_data=train_test_split(pat_df[['id','mgmt']],\n",
    "                                      stratify=pat_df['mgmt'],\n",
    "                                      random_state=100,\n",
    "                                      test_size=0.2\n",
    "                                      )\n",
    "print(f'Shape of train_data {train_data.shape}')\n",
    "print(f'Shape of test_data {test_data.shape}')\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Train and test data\n",
    "train_data.to_csv(config.MAIN_DIR+'results/train_data.csv',mode='w')\n",
    "test_data.to_csv(config.MAIN_DIR+'results/test_data.csv',mode='w')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **One Split Training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train and y_train was intialised\n",
      "X_val and y_val was intialised\n"
     ]
    }
   ],
   "source": [
    "# Loading the train npy\n",
    "train_T2_pos_one =np.load('D:/MGMT research project/data for one split/Patch 64x64/T2/pos_one_split_train_data_grayscale.npy')\n",
    "train_T2_neg_one =np.load('D:/MGMT research project/data for one split/Patch 64x64/T2/neg_one_split_train_data_grayscale.npy')\n",
    "X_train = np.append(train_T2_pos_one,train_T2_neg_one,axis=0)/255.0             # Normalising the array\n",
    "# y_train = [1]*40000 + [0]*40000\n",
    "y_train = [1]*len(train_T2_pos_one) + [0]*len(train_T2_neg_one)\n",
    "y_train = np.array(y_train)\n",
    "del train_T2_neg_one,train_T2_pos_one\n",
    "print(\"X_train and y_train was intialised\")\n",
    "\n",
    "# Loading the val npy\n",
    "val_T2_pos_one =np.load('D:/MGMT research project/data for one split/Patch 64x64/T2/pos_one_split_val_data_grayscale.npy')\n",
    "val_T2_neg_one =np.load('D:/MGMT research project/data for one split/Patch 64x64/T2/neg_one_split_val_data_grayscale.npy')\n",
    "X_val = np.append(val_T2_pos_one,val_T2_neg_one,axis=0)/255.0                   # Normalising the array\n",
    "# y_val = [1]*10000 + [0]*10000\n",
    "y_val = [1]*len(val_T2_pos_one) + [0]*len(val_T2_neg_one)\n",
    "y_val = np.array(y_val)\n",
    "del val_T2_neg_one,val_T2_pos_one\n",
    "print(\"X_val and y_val was intialised\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_training():\n",
    "    # Model intialisation with GPU\n",
    "    with tf.device('GPU:0'):\n",
    "        model = Sequential()\n",
    "        model.add(Conv2D(8, (3, 3), padding='same',\n",
    "                         input_shape=(block_h,block_w,1)))\n",
    "        # model.add(Conv2D(8, (3, 3), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(4, 4)))\n",
    "        BatchNormalization()\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Conv2D(8, (3, 3), padding='same'))\n",
    "        # model.add(Conv2D(16, (3, 3),padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(4,4)))\n",
    "        BatchNormalization()\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        # model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "        # model.add(LeakyReLU(alpha=0.1))\n",
    "        # model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "        # model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Flatten())  # Convert 3D feature map to 1D feature vector.\n",
    "\n",
    "        model.add(Dense(64))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        BatchNormalization()\n",
    "        model.add(Dropout(0.5))\n",
    "\n",
    "        model.add(Dense(8))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        BatchNormalization()\n",
    "        model.add(Dropout(0.3))\n",
    "        \n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compiling the model\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy',TruePositives(),TrueNegatives(),FalsePositives(),FalseNegatives(),AUC(),Recall(),\n",
    "                                                Precision()])\n",
    "\n",
    "    \n",
    "    # print(y_train[:50])\n",
    "\n",
    "    # Model Checkpoints\n",
    "    # checkpoint_filepath = config.MAIN_DIR+f'results/model checkpoints/model(k={cv+1})'\n",
    "    # model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath+'_epoch-{epoch:02d}_valloss-{val_loss:.4f}.ckpt',\n",
    "    #                                             # save_format='h5',\n",
    "    #                                             monitor='val_loss', \n",
    "    #                                             mode='min',\n",
    "    #                                             save_best_only=True)\n",
    "    # Reduce LRPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=2, min_lr=0.0001)\n",
    "\n",
    "    # Early stoping\n",
    "    early_stoping =EarlyStopping(monitor=\"val_loss\",patience=5,mode=\"min\") \n",
    "\n",
    "    # Model training\n",
    "    history = model.fit(X_train, y_train, batch_size=64, epochs=100,\n",
    "                        validation_data=(X_val, y_val), \n",
    "                        shuffle=True,\n",
    "                        callbacks=[reduce_lr,early_stoping])\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "18386/18386 [==============================] - 671s 27ms/step - loss: 0.6604 - accuracy: 0.5814 - true_positives: 324900.0000 - true_negatives: 359240.0000 - false_positives: 239176.0000 - false_negatives: 253356.0000 - auc: 0.6267 - recall: 0.5619 - precision: 0.5760 - val_loss: 0.7353 - val_accuracy: 0.4402 - val_true_positives: 93593.0000 - val_true_negatives: 50161.0000 - val_false_positives: 98351.0000 - val_false_negatives: 84487.0000 - val_auc: 0.4258 - val_recall: 0.5256 - val_precision: 0.4876 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "18386/18386 [==============================] - 458s 25ms/step - loss: 0.6385 - accuracy: 0.6088 - true_positives: 360005.0000 - true_negatives: 356337.0000 - false_positives: 242079.0000 - false_negatives: 218251.0000 - auc: 0.6682 - recall: 0.6226 - precision: 0.5979 - val_loss: 0.7369 - val_accuracy: 0.4535 - val_true_positives: 77772.0000 - val_true_negatives: 70332.0000 - val_false_positives: 78180.0000 - val_false_negatives: 100308.0000 - val_auc: 0.4377 - val_recall: 0.4367 - val_precision: 0.4987 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "18386/18386 [==============================] - 458s 25ms/step - loss: 0.6322 - accuracy: 0.6163 - true_positives: 361144.0000 - true_negatives: 364015.0000 - false_positives: 234401.0000 - false_negatives: 217112.0000 - auc: 0.6783 - recall: 0.6245 - precision: 0.6064 - val_loss: 0.7331 - val_accuracy: 0.4574 - val_true_positives: 99414.0000 - val_true_negatives: 49967.0000 - val_false_positives: 98545.0000 - val_false_negatives: 78666.0000 - val_auc: 0.4350 - val_recall: 0.5583 - val_precision: 0.5022 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "18386/18386 [==============================] - 460s 25ms/step - loss: 0.6286 - accuracy: 0.6225 - true_positives: 362078.0000 - true_negatives: 370450.0000 - false_positives: 227966.0000 - false_negatives: 216178.0000 - auc: 0.6857 - recall: 0.6262 - precision: 0.6136 - val_loss: 0.7306 - val_accuracy: 0.4911 - val_true_positives: 107910.0000 - val_true_negatives: 52488.0000 - val_false_positives: 96024.0000 - val_false_negatives: 70170.0000 - val_auc: 0.4701 - val_recall: 0.6060 - val_precision: 0.5291 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "18386/18386 [==============================] - 460s 25ms/step - loss: 0.6255 - accuracy: 0.6280 - true_positives: 360061.0000 - true_negatives: 378903.0000 - false_positives: 219513.0000 - false_negatives: 218195.0000 - auc: 0.6921 - recall: 0.6227 - precision: 0.6213 - val_loss: 0.7259 - val_accuracy: 0.4983 - val_true_positives: 115404.0000 - val_true_negatives: 47353.0000 - val_false_positives: 101159.0000 - val_false_negatives: 62676.0000 - val_auc: 0.4750 - val_recall: 0.6480 - val_precision: 0.5329 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "18386/18386 [==============================] - 462s 25ms/step - loss: 0.6237 - accuracy: 0.6315 - true_positives: 359460.0000 - true_negatives: 383606.0000 - false_positives: 214810.0000 - false_negatives: 218796.0000 - auc: 0.6961 - recall: 0.6216 - precision: 0.6259 - val_loss: 0.7271 - val_accuracy: 0.4936 - val_true_positives: 110312.0000 - val_true_negatives: 50885.0000 - val_false_positives: 97627.0000 - val_false_negatives: 67768.0000 - val_auc: 0.4679 - val_recall: 0.6195 - val_precision: 0.5305 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "18386/18386 [==============================] - 462s 25ms/step - loss: 0.6207 - accuracy: 0.6349 - true_positives: 358872.0000 - true_negatives: 388178.0000 - false_positives: 210238.0000 - false_negatives: 219384.0000 - auc: 0.7009 - recall: 0.6206 - precision: 0.6306 - val_loss: 0.7347 - val_accuracy: 0.4872 - val_true_positives: 98411.0000 - val_true_negatives: 60702.0000 - val_false_positives: 87810.0000 - val_false_negatives: 79669.0000 - val_auc: 0.4747 - val_recall: 0.5526 - val_precision: 0.5285 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "18386/18386 [==============================] - 461s 25ms/step - loss: 0.6148 - accuracy: 0.6424 - true_positives: 363953.0000 - true_negatives: 391949.0000 - false_positives: 206467.0000 - false_negatives: 214303.0000 - auc: 0.7098 - recall: 0.6294 - precision: 0.6380 - val_loss: 0.7395 - val_accuracy: 0.4863 - val_true_positives: 96971.0000 - val_true_negatives: 61860.0000 - val_false_positives: 86652.0000 - val_false_negatives: 81109.0000 - val_auc: 0.4769 - val_recall: 0.5445 - val_precision: 0.5281 - lr: 2.0000e-04\n",
      "Epoch 9/100\n",
      "18386/18386 [==============================] - 457s 25ms/step - loss: 0.6133 - accuracy: 0.6437 - true_positives: 362373.0000 - true_negatives: 395082.0000 - false_positives: 203334.0000 - false_negatives: 215883.0000 - auc: 0.7123 - recall: 0.6267 - precision: 0.6406 - val_loss: 0.7478 - val_accuracy: 0.4843 - val_true_positives: 90420.0000 - val_true_negatives: 67755.0000 - val_false_positives: 80757.0000 - val_false_negatives: 87660.0000 - val_auc: 0.4851 - val_recall: 0.5077 - val_precision: 0.5282 - lr: 2.0000e-04\n",
      "Epoch 10/100\n",
      "18386/18386 [==============================] - 450s 24ms/step - loss: 0.6119 - accuracy: 0.6456 - true_positives: 361925.0000 - true_negatives: 397741.0000 - false_positives: 200675.0000 - false_negatives: 216331.0000 - auc: 0.7142 - recall: 0.6259 - precision: 0.6433 - val_loss: 0.7576 - val_accuracy: 0.4807 - val_true_positives: 84002.0000 - val_true_negatives: 72980.0000 - val_false_positives: 75532.0000 - val_false_negatives: 94078.0000 - val_auc: 0.4830 - val_recall: 0.4717 - val_precision: 0.5265 - lr: 1.0000e-04\n"
     ]
    }
   ],
   "source": [
    "model = model_training()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Cross Validation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross Validation --> Splitting the data and saving the train and val indices of each cv in pickle in dictionary\n",
    "def patient_val_split(df:pd.DataFrame,cv: int) -> dict:\n",
    "    dict_cv={}\n",
    "    skf=StratifiedKFold(n_splits=cv, random_state=100, shuffle=True)\n",
    "\n",
    "    for i,(train_idx,val_idx) in enumerate(skf.split(df[['id']],df['mgmt'])):\n",
    "        dict_cv[i+1]=[train_idx,val_idx]\n",
    "    \n",
    "    pkl.dump(dict_cv,open(config.MAIN_DIR+'results/cross_validation_indexes.pkl','wb'))\n",
    "    print(\"Splitting indices is completed\")\n",
    "    return dict_cv\n",
    "    \n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Splitting indices is completed\n"
     ]
    }
   ],
   "source": [
    "# Cross validating data and storing in pickle\n",
    "cv_idx_dict = patient_val_split(df=train_data,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11\n"
     ]
    }
   ],
   "source": [
    "x=91/8\n",
    "print(int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fubction definition --> All patients of training/validation were appended to one train list and val list \n",
    "# for x and y so that they be used in training\n",
    "def making_all_arrays_to_list(idx: list,x: list,y: list):\n",
    "    for row in tqdm(idx):\n",
    "        if train_data['mgmt'].iloc[row]==0:\n",
    "            label = 'MGMT_negative/'\n",
    "        else: label = 'MGMT_positive/'\n",
    "        pkl_file = pkl.load(open(config.MAIN_DIR+'preprocessed/64X64/'+label+train_data['id'].iloc[row],'rb'))\n",
    "        for arr in pkl_file:\n",
    "            x.append(arr)\n",
    "            y.append(train_data['mgmt'].iloc[row])\n",
    "    # x=np.array(x)\n",
    "    # y=np.array(y)\n",
    "def randomisation(list_large: list, list_small: list):\n",
    "    indices = []\n",
    "    temp, j = 0, 0\n",
    "    for i in range(len(list_large)):\n",
    "        indices.append(list_large[i])\n",
    "        temp+=1\n",
    "\n",
    "        if(temp == int(len(list_large)/len(list_small))) and j<len(list_small):\n",
    "            # print(len(list_small))\n",
    "            # print(j)\n",
    "            indices.append(list_small[j])\n",
    "            j+=1\n",
    "            temp=0\n",
    "\n",
    "        if len(indices) % batch_size == 0:\n",
    "            seed_value = 1\n",
    "            rn.seed(seed_value)\n",
    "            iteration = int(len(indices) / batch_size)\n",
    "            start_idx = (iteration-1)*batch_size\n",
    "            rn.shuffle(indices[start_idx:start_idx+batch_size-1])\n",
    "            # break\n",
    "\n",
    "    return indices\n",
    "\n",
    "\n",
    "\n",
    "def data_stratified_indices(l1:list):\n",
    "    # Lenght list y\n",
    "    total_samples = len(l1)\n",
    "    idx_0, idx_1 = [], []\n",
    "\n",
    "    for idx in range(total_samples):\n",
    "        if l1[idx]==0: idx_0.append(idx)\n",
    "        else: idx_1.append(idx)\n",
    "\n",
    "    if len(idx_0) > len(idx_1): \n",
    "        return randomisation(list_large= idx_0,list_small= idx_1)\n",
    "    else :\n",
    "        return randomisation(list_large= idx_1,list_small= idx_0)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "# Function definintion --> It calls making_all_arrays_to_list() for train and val\n",
    "def data_arr(indexes: list):\n",
    "    train_idx,val_idx = indexes\n",
    "    train_x, train_y = [], []\n",
    "    val_x, val_y = [], []\n",
    "\n",
    "    # fetching Train Data \n",
    "    making_all_arrays_to_list(idx = train_idx,x = train_x,y = train_y)\n",
    "    print(\"Training data was fetched\")\n",
    "    # fetching Val Data \n",
    "    making_all_arrays_to_list(idx = val_idx,x = val_x,y = val_y)\n",
    "    print(\"Validation data was fetched\")\n",
    "    train_stratify_index = data_stratified_indices(l1= train_y)\n",
    "    # val_stratify_index = data_stratified_indices(l1= val_y)\n",
    "    print(\"stratify indices done\")\n",
    "    train_x = list(train_x[i] for i in train_stratify_index)\n",
    "    train_y = list(train_y[i] for i in train_stratify_index)\n",
    "    print('data_arr done')\n",
    "    return train_x, train_y, val_x, val_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpu_device = tf.config.list_physical_devices('GPU')[0]\n",
    "gpu_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition --> Make model checkpoint folder empty\n",
    "def make_ckpt_folder_empty(folder_path):\n",
    "    # Check if the folder exists\n",
    "    if os.path.exists(folder_path):\n",
    "        # List all files and directories in the folder\n",
    "        contents = os.listdir(folder_path)\n",
    "        \n",
    "        # Check if the folder is empty\n",
    "        if len(contents) == 0:\n",
    "            print(f\"The folder '{folder_path}' is already empty.\")\n",
    "        else:\n",
    "            # Deletes the folder tree\n",
    "            shutil.rmtree(folder_path)\n",
    "\n",
    "            # Recreate the folder\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"The folder '{folder_path}' has been emptied.\")\n",
    "    else:\n",
    "        print(f\"The folder '{folder_path}' does not exist.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Funtion Defination --> train the model and stores the history\n",
    "# import h5py\n",
    "def model_training(data_idx: list):\n",
    "    cv, cv_indexes = data_idx\n",
    "\n",
    "    # Model intialisation with GPU\n",
    "    with tf.device('GPU:0'):\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, (3, 3), padding='same',input_shape=(block_h,block_w,1)))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "        model.add(Dropout(0.1))\n",
    "\n",
    "        model.add(Flatten())  # Convert 3D feature map to 1D feature vector.\n",
    "\n",
    "        model.add(Dense(1096))\n",
    "        model.add(LeakyReLU(alpha=0.1))\n",
    "        model.add(Dropout(0.1))\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        # Compiling the model\n",
    "        model.compile(loss='binary_crossentropy',\n",
    "                    optimizer='adam', metrics=['accuracy',TruePositives(),TrueNegatives(),FalsePositives(),FalseNegatives(),AUC(),Recall(),\n",
    "                                                Precision()])\n",
    "\n",
    "    # Selecting the data from train_idx and test_idx\n",
    "    X_train, y_train, X_val, y_val = data_arr(indexes = cv_indexes)\n",
    "    print(\"all data fetched\")\n",
    "    X_train=np.array(X_train)\n",
    "    y_train=np.array(y_train)\n",
    "    X_val=np.array(X_val)\n",
    "    y_val=np.array(y_val)\n",
    "    # print(y_train[:50])\n",
    "\n",
    "    # Model Checkpoints\n",
    "    checkpoint_filepath = config.MAIN_DIR+f'results/model checkpoints/model(k={cv+1})'\n",
    "    model_checkpoint_callback = ModelCheckpoint(filepath=checkpoint_filepath+'_epoch-{epoch:02d}_valloss-{val_loss:.4f}.ckpt',\n",
    "                                                # save_format='h5',\n",
    "                                                monitor='val_loss', \n",
    "                                                mode='min',\n",
    "                                                save_best_only=True)\n",
    "    # Reduce LRPlateau\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,patience=3, min_lr=0.0001)\n",
    "\n",
    "    # Early stoping\n",
    "    early_stoping =EarlyStopping(monitor=\"val_loss\",patience=5,mode=\"min\") \n",
    "    # Model training\n",
    "    print(f\"Model Training for {cv} was started...\")\n",
    "    \n",
    "    history = model.fit(X_train, y_train, batch_size=batch_size, epochs=epoch,\n",
    "                        validation_data=(X_val, y_val,), shuffle=True,callbacks=[model_checkpoint_callback,reduce_lr,early_stoping])\n",
    "    \n",
    "    # Stores the history in pickle\n",
    "    pkl.dump(history.history,open(config.MAIN_DIR+f'results/history/history_k={cv}.pkl','wb'))\n",
    "    print(f\"Model Training for cv-{cv} was completed....\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The folder 'D:/MGMT research project/results/model checkpoints' is already empty.\n",
      "Cross Validation k=1 was started....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [02:21<00:00,  2.24s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data was fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [00:17<00:00,  2.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data was fetched\n",
      "stratify indices done\n",
      "data_arr done\n",
      "all data fetched\n",
      "Model Training for 1 was started...\n",
      "Epoch 1/100\n",
      "36603/36603 [==============================] - ETA: 0s - loss: 0.4347 - accuracy: 0.7711 - true_positives_2: 1816345.0000 - true_negatives_2: 1796407.0000 - false_positives_2: 551561.0000 - false_negatives_2: 520871.0000 - auc_2: 0.8729 - recall_2: 0.7771 - precision_2: 0.7671"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/MGMT research project/results/model checkpoints\\model(k=2)_epoch-01_valloss-1.9183.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/MGMT research project/results/model checkpoints\\model(k=2)_epoch-01_valloss-1.9183.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36603/36603 [==============================] - 1005s 27ms/step - loss: 0.4347 - accuracy: 0.7711 - true_positives_2: 1816345.0000 - true_negatives_2: 1796407.0000 - false_positives_2: 551561.0000 - false_negatives_2: 520871.0000 - auc_2: 0.8729 - recall_2: 0.7771 - precision_2: 0.7671 - val_loss: 1.9183 - val_accuracy: 0.3962 - val_true_positives_2: 88606.0000 - val_true_negatives_2: 108954.0000 - val_false_positives_2: 131622.0000 - val_false_negatives_2: 169442.0000 - val_auc_2: 0.3702 - val_recall_2: 0.3434 - val_precision_2: 0.4023 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "36603/36603 [==============================] - ETA: 0s - loss: 0.2937 - accuracy: 0.8657 - true_positives_2: 2024921.0000 - true_negatives_2: 2030865.0000 - false_positives_2: 317103.0000 - false_negatives_2: 312295.0000 - auc_2: 0.9472 - recall_2: 0.8664 - precision_2: 0.8646"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as _jit_compiled_convolution_op, _jit_compiled_convolution_op, _jit_compiled_convolution_op while saving (showing 3 of 3). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/MGMT research project/results/model checkpoints\\model(k=2)_epoch-02_valloss-1.7778.ckpt\\assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: D:/MGMT research project/results/model checkpoints\\model(k=2)_epoch-02_valloss-1.7778.ckpt\\assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36603/36603 [==============================] - 977s 27ms/step - loss: 0.2937 - accuracy: 0.8657 - true_positives_2: 2024921.0000 - true_negatives_2: 2030865.0000 - false_positives_2: 317103.0000 - false_negatives_2: 312295.0000 - auc_2: 0.9472 - recall_2: 0.8664 - precision_2: 0.8646 - val_loss: 1.7778 - val_accuracy: 0.4204 - val_true_positives_2: 105715.0000 - val_true_negatives_2: 103930.0000 - val_false_positives_2: 136646.0000 - val_false_negatives_2: 152333.0000 - val_auc_2: 0.3996 - val_recall_2: 0.4097 - val_precision_2: 0.4362 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "36603/36603 [==============================] - 986s 27ms/step - loss: 0.2633 - accuracy: 0.8834 - true_positives_2: 2066154.0000 - true_negatives_2: 2072543.0000 - false_positives_2: 275425.0000 - false_negatives_2: 271062.0000 - auc_2: 0.9582 - recall_2: 0.8840 - precision_2: 0.8824 - val_loss: 2.1071 - val_accuracy: 0.4050 - val_true_positives_2: 103578.0000 - val_true_negatives_2: 98346.0000 - val_false_positives_2: 142230.0000 - val_false_negatives_2: 154470.0000 - val_auc_2: 0.3774 - val_recall_2: 0.4014 - val_precision_2: 0.4214 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "36603/36603 [==============================] - 979s 27ms/step - loss: 0.2482 - accuracy: 0.8920 - true_positives_2: 2085967.0000 - true_negatives_2: 2093047.0000 - false_positives_2: 254921.0000 - false_negatives_2: 251249.0000 - auc_2: 0.9631 - recall_2: 0.8925 - precision_2: 0.8911 - val_loss: 2.2683 - val_accuracy: 0.3622 - val_true_positives_2: 80180.0000 - val_true_negatives_2: 100418.0000 - val_false_positives_2: 140158.0000 - val_false_negatives_2: 177868.0000 - val_auc_2: 0.3388 - val_recall_2: 0.3107 - val_precision_2: 0.3639 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "36603/36603 [==============================] - 986s 27ms/step - loss: 0.2386 - accuracy: 0.8973 - true_positives_2: 2098371.0000 - true_negatives_2: 2105709.0000 - false_positives_2: 242259.0000 - false_negatives_2: 238845.0000 - auc_2: 0.9661 - recall_2: 0.8978 - precision_2: 0.8965 - val_loss: 2.3025 - val_accuracy: 0.3730 - val_true_positives_2: 90976.0000 - val_true_negatives_2: 95029.0000 - val_false_positives_2: 145547.0000 - val_false_negatives_2: 167072.0000 - val_auc_2: 0.3461 - val_recall_2: 0.3526 - val_precision_2: 0.3846 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "36603/36603 [==============================] - 982s 27ms/step - loss: 0.1533 - accuracy: 0.9361 - true_positives_2: 2188759.0000 - true_negatives_2: 2196944.0000 - false_positives_2: 151024.0000 - false_negatives_2: 148457.0000 - auc_2: 0.9855 - recall_2: 0.9365 - precision_2: 0.9355 - val_loss: 2.5168 - val_accuracy: 0.3866 - val_true_positives_2: 97085.0000 - val_true_negatives_2: 95659.0000 - val_false_positives_2: 144917.0000 - val_false_negatives_2: 160963.0000 - val_auc_2: 0.3542 - val_recall_2: 0.3762 - val_precision_2: 0.4012 - lr: 2.0000e-04\n",
      "Epoch 7/100\n",
      "36603/36603 [==============================] - 980s 27ms/step - loss: 0.1368 - accuracy: 0.9441 - true_positives_2: 2207349.0000 - true_negatives_2: 2215855.0000 - false_positives_2: 132113.0000 - false_negatives_2: 129867.0000 - auc_2: 0.9883 - recall_2: 0.9444 - precision_2: 0.9435 - val_loss: 2.4724 - val_accuracy: 0.4040 - val_true_positives_2: 108575.0000 - val_true_negatives_2: 92860.0000 - val_false_positives_2: 147716.0000 - val_false_negatives_2: 149473.0000 - val_auc_2: 0.3658 - val_recall_2: 0.4208 - val_precision_2: 0.4236 - lr: 2.0000e-04\n",
      "Model Training for cv-1 was completed....\n",
      "Cross Validation k=2 was started....\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63/63 [02:28<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data was fetched\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7/7 [06:42<00:00, 57.45s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation data was fetched\n",
      "stratify indices done\n",
      "data_arr done\n",
      "all data fetched\n",
      "Model Training for 2 was started...\n",
      "Epoch 1/100\n",
      "36362/36362 [==============================] - ETA: 0s - loss: 0.4489 - accuracy: 0.7610 - true_positives_3: 1792221.0000 - true_negatives_3: 1749486.0000 - false_positives_3: 603858.0000 - false_negatives_3: 508707.0000 - auc_3: 0.8629 - recall_3: 0.7789 - precision_3: 0.7480"
     ]
    },
    {
     "ename": "InternalError",
     "evalue": "Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInternalError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[26], line 10\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m cv \u001b[39min\u001b[39;00m cv_idx_dict:\n\u001b[0;32m      9\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCross Validation k=\u001b[39m\u001b[39m{\u001b[39;00mcv\u001b[39m}\u001b[39;00m\u001b[39m was started....\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 10\u001b[0m     model_training([cv,cv_idx_dict[cv]])\n\u001b[0;32m     12\u001b[0m \u001b[39m# End time\u001b[39;00m\n\u001b[0;32m     13\u001b[0m end\u001b[39m=\u001b[39mtime\u001b[39m.\u001b[39mtime()\n",
      "Cell \u001b[1;32mIn[23], line 61\u001b[0m, in \u001b[0;36mmodel_training\u001b[1;34m(data_idx)\u001b[0m\n\u001b[0;32m     58\u001b[0m \u001b[39m# Model training\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mModel Training for \u001b[39m\u001b[39m{\u001b[39;00mcv\u001b[39m}\u001b[39;00m\u001b[39m was started...\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m---> 61\u001b[0m history \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mfit(X_train, y_train, batch_size\u001b[39m=\u001b[39;49mbatch_size, epochs\u001b[39m=\u001b[39;49mepoch,\n\u001b[0;32m     62\u001b[0m                     validation_data\u001b[39m=\u001b[39;49m(X_val, y_val,), shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,callbacks\u001b[39m=\u001b[39;49m[model_checkpoint_callback,reduce_lr,early_stoping])\n\u001b[0;32m     64\u001b[0m \u001b[39m# Stores the history in pickle\u001b[39;00m\n\u001b[0;32m     65\u001b[0m pkl\u001b[39m.\u001b[39mdump(history\u001b[39m.\u001b[39mhistory,\u001b[39mopen\u001b[39m(config\u001b[39m.\u001b[39mMAIN_DIR\u001b[39m+\u001b[39m\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mresults/history/history_k=\u001b[39m\u001b[39m{\u001b[39;00mcv\u001b[39m}\u001b[39;00m\u001b[39m.pkl\u001b[39m\u001b[39m'\u001b[39m,\u001b[39m'\u001b[39m\u001b[39mwb\u001b[39m\u001b[39m'\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\Suraj\\.conda\\envs\\tfgpu\\lib\\site-packages\\keras\\utils\\traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[0;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[0;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Suraj\\.conda\\envs\\tfgpu\\lib\\site-packages\\tensorflow\\python\\framework\\constant_op.py:102\u001b[0m, in \u001b[0;36mconvert_to_eager_tensor\u001b[1;34m(value, ctx, dtype)\u001b[0m\n\u001b[0;32m    100\u001b[0m     dtype \u001b[39m=\u001b[39m dtypes\u001b[39m.\u001b[39mas_dtype(dtype)\u001b[39m.\u001b[39mas_datatype_enum\n\u001b[0;32m    101\u001b[0m ctx\u001b[39m.\u001b[39mensure_initialized()\n\u001b[1;32m--> 102\u001b[0m \u001b[39mreturn\u001b[39;00m ops\u001b[39m.\u001b[39;49mEagerTensor(value, ctx\u001b[39m.\u001b[39;49mdevice_name, dtype)\n",
      "\u001b[1;31mInternalError\u001b[0m: Failed copying input tensor from /job:localhost/replica:0/task:0/device:CPU:0 to /job:localhost/replica:0/task:0/device:GPU:0 in order to run _EagerConst: Dst tensor is not initialized."
     ]
    }
   ],
   "source": [
    "# Empting the model checkpoint folder before running new cross validation\n",
    "make_ckpt_folder_empty(config.MAIN_DIR+'results/model checkpoints')\n",
    "\n",
    "# Starts time\n",
    "start=time.time()\n",
    "\n",
    "# Cross validation starts here\n",
    "for cv in cv_idx_dict:\n",
    "    print(f\"Cross Validation k={cv} was started....\")\n",
    "    model_training([cv,cv_idx_dict[cv]])\n",
    "\n",
    "# End time\n",
    "end=time.time()\n",
    "print(f'Total Time taken by cross validation - {end-start}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# x=[1,2,3,4,53,2,1]\n",
    "# rn.shuffle(x[0:5])\n",
    "# x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Sequential()\n",
    "\n",
    "# model.add(Conv2D(32, (3, 3), padding='same',input_shape=(32,32,1)))\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "# model.add(Dropout(0.1))\n",
    "\n",
    "# model.add(Flatten())  # Convert 3D feature map to 1D feature vector.\n",
    "\n",
    "# model.add(Dense(1096))\n",
    "# model.add(LeakyReLU(alpha=0.1))\n",
    "# model.add(Dropout(0.1))\n",
    "# model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint\n",
    "# checkpoint = tf.train.Checkpoint(model=model)\n",
    "# ckpt_load_path = 'model(k=9)_epoch-06_valloss-0.0000.ckpt'\n",
    "# checkpoint.restore(config.MAIN_DIR+'results/model checkpoints/'+ckpt_load_path)\n",
    "\n",
    "# print(f\"Model restored from {ckpt_load_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b9603ab75704494434c4abe56997ef3bb46c839483ec68f7dba80f8b5009106"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
