{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Import libraries and define variables\n",
    "import os\n",
    "import shutil\n",
    "import glob\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import nibabel as nib\n",
    "import numpy as np\n",
    "import pickle\n",
    "import cv2\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import MaxPooling2D, Dense, Flatten, Dropout, LeakyReLU, Activation\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.layers.convolutional import Conv2D\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import config\n",
    "\n",
    "# Define the modalities and classifications\n",
    "modalities = ['t1', 't1ce', 't2', 'flair']\n",
    "classifications = ['MGMT_positive', 'MGMT_negative']\n",
    "\n",
    "# Define patch size and stride\n",
    "block_h, block_w = (32, 32)\n",
    "stride = 2\n",
    "\n",
    "# Interpolated image dimestions\n",
    "inter_dim = (110, 90)\n",
    "\n",
    "# Define epoch\n",
    "epoch = 5\n",
    "batch_size = 32\n",
    "\n",
    "# Define paths to the BraTS dataset folders\n",
    "path = config.DATASET_PATH\n",
    "\n",
    "PATH = config.MAIN_DIR + 'Data/'\n",
    "Org_Dir = PATH + 'Original_Data_Backup/'\n",
    "Work_Dir = PATH + 'Working_Data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Defination --> Load the data\n",
    "\n",
    "def load_data():\n",
    "    print('Loading data...')\n",
    "    pickle_in = open(Work_Dir + 'X.pickle', 'rb')\n",
    "    X = pickle.load(pickle_in) # Loading Features\n",
    "\n",
    "    pickle_in = open(Work_Dir + 'y.pickle', 'rb')\n",
    "    y = pickle.load(pickle_in) # Loading Labels \n",
    "\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Funtion Defination --> Model defination\n",
    "def Model(img_h, img_w, X_train, y_train, X_test, y_test, epoch):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "              input_shape=(img_h, img_w, 1)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    # Pool_size is the size of filter.\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Flatten())  # Convert 3D feature map to 1D feature vector.\n",
    "\n",
    "    model.add(Dense(1096))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    model.optimizer.learning_rate = 0.0001\n",
    "\n",
    "    # Training the model & storing its detail in the variable h\n",
    "    history = model.fit(X_train, to_categorical(y_train), batch_size=128, epochs=epoch, verbose=2,\n",
    "                        validation_data=(X_test, to_categorical(y_test)), shuffle=True)\n",
    "\n",
    "    # Evaluating performance of the Model\n",
    "    test_loss, test_acc = model.evaluate(X_test, to_categorical(y_test))\n",
    "    results.append(test_acc)\n",
    "    print('Test Loss is: ', test_loss, '\\nTest Accuracy is: ', test_acc)\n",
    "\n",
    "    # Saving the model in hierarchical Data Formate (HDF)\n",
    "    model.save(path + 'Outputs/latest_model'+'.h5')\n",
    "\n",
    "    return history, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Funtion Defination --> Model defination\n",
    "def Model_datagen(img_h, img_w, train_dataset, val_dataset, epoch):\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(32, (3, 3), padding='same',\n",
    "              input_shape=(img_h, img_w, 1)))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    # Pool_size is the size of filter.\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Conv2D(48, (3, 3), padding='same'))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(MaxPooling2D(pool_size=(3, 3)))\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    model.add(Flatten())  # Convert 3D feature map to 1D feature vector.\n",
    "\n",
    "    model.add(Dense(1096))\n",
    "    model.add(LeakyReLU(alpha=0.1))\n",
    "    model.add(Dropout(0.1))\n",
    "    model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    # Compiling the model\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam', metrics=['accuracy'])\n",
    "    model.optimizer.learning_rate = 0.0001\n",
    "\n",
    "    # Training the model & storing its detail in the variable h\n",
    "    history = model.fit(train_dataset, batch_size=128, epochs=epoch, verbose=2,\n",
    "                        validation_data=(val_dataset), shuffle=True)\n",
    "\n",
    "    # Evaluating performance of the Model\n",
    "    # test_loss, test_acc = model.evaluate(X_test, to_categorical(y_test))\n",
    "    # results.append(test_acc)\n",
    "    # print('Test Loss is: ', test_loss, '\\nTest Accuracy is: ', test_acc)\n",
    "\n",
    "    # Saving the model in hierarchical Data Formate (HDF)\n",
    "    model.save(config.MAIN_DIR + 'Outputs/latest_model'+'.h5')\n",
    "    pickle.dump(history, open(config.MAIN_DIR + 'Outputs/history.pickle', 'wb'))\n",
    "\n",
    "    return history, model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function Definition --> For Saving Validation Accuracy in the file\n",
    "\n",
    "def Saving_Test_Accuracy():\n",
    "    import numpy as np\n",
    "\n",
    "    print('Saving Test Accuracy...')\n",
    "\n",
    "    file = open(Result_File, \"a\")\n",
    "    file.write(case)\n",
    "    file.write(':\\n======')\n",
    "    file.write('\\nAll Test Accuracy:\\t')\n",
    "\n",
    "    for ans in results:\n",
    "        file.write(str(ans) + ', ')\n",
    "\n",
    "    file.write('\\nMean Test Accuracy:\\t' + str(np.mean(results)))\n",
    "    file.write('\\nMin Test Accuracy:\\t' + str(np.min(results)))\n",
    "    file.write('\\nMax Test Accuracy:\\t' + str(np.max(results)))\n",
    "    file.write('\\n\\n\\n')\n",
    "    file.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 03:33:17.524804: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-05-01 03:33:17.526282: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-01 03:33:21.728188: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2023-05-01 03:33:22.202597: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n",
      "2023-05-01 03:37:41.747620: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15036/15036 - 270s - loss: 0.6035 - accuracy: 0.6581 - val_loss: 0.5175 - val_accuracy: 0.7305 - 270s/epoch - 18ms/step\n",
      "Epoch 2/5\n",
      "15036/15036 - 220s - loss: 0.4943 - accuracy: 0.7496 - val_loss: 0.4658 - val_accuracy: 0.7674 - 220s/epoch - 15ms/step\n",
      "Epoch 3/5\n",
      "15036/15036 - 215s - loss: 0.4260 - accuracy: 0.7947 - val_loss: 0.3610 - val_accuracy: 0.8334 - 215s/epoch - 14ms/step\n",
      "Epoch 4/5\n",
      "15036/15036 - 213s - loss: 0.3810 - accuracy: 0.8221 - val_loss: 0.3340 - val_accuracy: 0.8473 - 213s/epoch - 14ms/step\n",
      "Epoch 5/5\n",
      "15036/15036 - 212s - loss: 0.3484 - accuracy: 0.8407 - val_loss: 0.2788 - val_accuracy: 0.8777 - 212s/epoch - 14ms/step\n",
      "6683/6683 [==============================] - 35s 5ms/step - loss: 0.2788 - accuracy: 0.8777\n",
      "Test Loss is:  0.27877816557884216 \n",
      "Test Accuracy is:  0.8777450323104858\n"
     ]
    }
   ],
   "source": [
    "# Main cell to execute the functions\n",
    "\n",
    "X, y = load_data()\n",
    "\n",
    "results = []\n",
    "\n",
    "# Split the data into train and test\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "# Training the model\n",
    "history, model = Model(block_h, block_w, X_train, y_train, X_test, y_test, epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# new main function\n",
    "\n",
    "# Create Image Data Generator \n",
    "image_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1./255)\n",
    "\n",
    "df = pd.read_csv('step2_file_paths.csv')\n",
    "\n",
    "# Split the data into train and test\n",
    "train_df, test_df = train_test_split(df, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "training_set = image_gen.flow_from_dataframe(dataframe= train_df,\n",
    "                                                 x_col=\"images\", y_col=\"labels\",\n",
    "                                                 class_mode=\"binary\",\n",
    "                                                 target_size=(block_h,block_w), batch_size=batch_size)\n",
    "    \n",
    "validation_set = image_gen.flow_from_dataframe(dataframe=test_df,\n",
    "                                                 x_col=\"images\", y_col=\"labels\",\n",
    "                                                 class_mode=\"binary\",\n",
    "                                                 target_size=(block_h,block_w), batch_size=batch_size)\n",
    "\n",
    "# Training the model\n",
    "history, model = Model_datagen(block_h, block_w, training_set, validation_set, epoch)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1b9603ab75704494434c4abe56997ef3bb46c839483ec68f7dba80f8b5009106"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
